name: Summarize new issues
run-name: Summarize ${{ github.event.issue.title }}[${{ github.event.issue.number }}]

on:
  issues:
    types: [opened]

env:
  SUMMARY_LANGUAGE: ${{ vars.SUMMARY_LANGUAGE || 'cn' }}

jobs:
  summary:
    runs-on: ubuntu-latest
    permissions:
      issues: write
      models: read
      contents: read

    services:
      crawl4ai:
        image: unclecode/crawl4ai:0.7.3
        ports:
          - 11235:11235
        options: --shm-size=1g

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Validate URL format
        id: urlValidation
        run: |
          URL="${{ github.event.issue.body }}"
          if [[ $URL =~ ^https?:// ]]; then
            echo "valid=true" >> $GITHUB_OUTPUT
            echo "url=$URL" >> $GITHUB_OUTPUT
          else
            echo "valid=false" >> $GITHUB_OUTPUT
            echo "URL validation failed: '$URL' is not a valid HTTP/HTTPS URL"
          fi
      
      - name: Submit crawl job to Crawl4AI
        id: crawlSubmit
        if: steps.urlValidation.outputs.valid == 'true'
        uses: fjogeleit/http-request-action@v1
        with:
          url: 'http://localhost:11235/crawl'
          method: 'POST'
          contentType: 'application/json'
          data: '{"urls": ["${{ steps.urlValidation.outputs.url }}"], "priority": 10}'
          timeout: 10000
        continue-on-error: true

      - name: Wait for crawl completion
        id: crawlResult
        if: steps.crawlSubmit.outcome == 'success' && steps.crawlSubmit.outputs.response != ''
        run: |
          python3 << 'PYTHON_SCRIPT'
          import json
          import os
          import sys
          import time
          import requests

          def wait_for_completion():
              """Wait for Crawl4AI task completion with robust error handling"""
              
              raw_response = os.environ.get('RAW_RESPONSE', '')
              print(f"Raw response from crawl submit: {raw_response}")
              
              if not raw_response:
                  print("Error: Empty response from crawl submit")
                  return None
              
              try:
                  response_data = json.loads(raw_response)
                  task_id = response_data.get('task_id')
                  
                  if not task_id:
                      print(f"Error: No task_id in response: {list(response_data.keys())}")
                      return None
                  
                  print(f"Successfully extracted task_id: {task_id}")
                  
                  # Poll for completion (max 60 seconds)
                  for attempt in range(1, 13):
                      try:
                          response = requests.get(f"http://localhost:11235/task/{task_id}", timeout=10)
                          
                          if response.status_code != 200:
                              print(f"Attempt {attempt}: HTTP {response.status_code}")
                              time.sleep(5)
                              continue
                          
                          task_data = response.json()
                          status = task_data.get('status', 'unknown')
                          print(f"Attempt {attempt}: Task status: {status}")
                          
                          if status == 'completed':
                              print("Task completed successfully")
                              return task_data
                          elif status == 'failed':
                              error_msg = task_data.get('error', 'No error details')
                              print(f"Task failed: {error_msg}")
                              return None
                          
                          time.sleep(5)
                          
                      except requests.exceptions.RequestException as e:
                          print(f"Attempt {attempt}: Request failed: {e}")
                          time.sleep(5)
                          continue
                      except json.JSONDecodeError as e:
                          print(f"Attempt {attempt}: Invalid JSON response: {e}")
                          time.sleep(5)
                          continue
                  
                  print("Task timeout after 60 seconds")
                  return None
                  
              except json.JSONDecodeError as e:
                  print(f"Error parsing crawl submit response: {e}")
                  return None
              except Exception as e:
                  print(f"Unexpected error: {e}")
                  return None

          def set_github_output(name, value):
              """Set GitHub Actions output"""
              github_output = os.environ.get('GITHUB_OUTPUT')
              if github_output:
                  with open(github_output, 'a', encoding='utf-8') as f:
                      delimiter = "RESPONSE_EOF"
                      f.write(f"{name}<<{delimiter}\n")
                      f.write(value)
                      f.write(f"\n{delimiter}\n")

          try:
              result = wait_for_completion()
              if result:
                  set_github_output('response', json.dumps(result))
                  print("Successfully set response output")
              else:
                  print("Failed to get valid response, continuing with empty result")
                  set_github_output('response', '{}')
          except Exception as e:
              print(f"Error in wait_for_completion: {e}")
              set_github_output('response', '{}')
              
          PYTHON_SCRIPT
        env:
          RAW_RESPONSE: ${{ steps.crawlSubmit.outputs.response }}
        continue-on-error: true

      - name: Determine prompt file
        id: promptFile
        run: |
          if [[ "${{ env.SUMMARY_LANGUAGE }}" == "en" ]]; then
            echo "file=./.github/prompts/summary-system-en.txt" >> $GITHUB_OUTPUT
          else
            echo "file=./.github/prompts/summary-system-cn.txt" >> $GITHUB_OUTPUT
          fi

      - name: Rate limiting delay
        run: sleep 2

      - name: Truncate content for token limit
        id: truncateContent
        run: |
          python3 << 'PYTHON_SCRIPT'
          import json
          import os
          import sys

          def extract_content():
              """Extract content from Crawl4AI response with robust error handling"""
              
              # Get environment variables
              crawl_outcome = os.environ.get('CRAWL_OUTCOME', '')
              raw_response = os.environ.get('RAW_RESPONSE', '')
              fallback_url = os.environ.get('FALLBACK_URL', '')
              fallback_title = os.environ.get('FALLBACK_TITLE', '')
              
              print(f"Crawl4AI completion outcome: {crawl_outcome}")
              
              content = ""
              
              # Try to extract content from successful response
              if crawl_outcome == 'success' and raw_response:
                  try:
                      response_data = json.loads(raw_response)
                      print("Successfully parsed JSON response")
                      
                      # Try multiple content fields from Crawl4AI
                      result = response_data.get('result', {})
                      content = (
                          result.get('fit_markdown') or
                          result.get('markdown') or
                          result.get('cleaned_html') or
                          result.get('raw_html') or
                          ""
                      )
                      
                      if content:
                          print("Successfully extracted content from Crawl4AI response")
                      else:
                          print("Warning: No content found in response data")
                          
                  except json.JSONDecodeError as e:
                      print(f"Warning: Failed to parse JSON response: {e}")
                      print(f"Response preview: {raw_response[:200]}...")
                  except Exception as e:
                      print(f"Warning: Unexpected error processing response: {e}")
              else:
                  print("Warning: Crawl4AI completion step was not successful or response is empty")
              
              # Use fallback if no content extracted
              if not content:
                  print("Using fallback content due to extraction failure")
                  content = f"URL: {fallback_url}\nTitle: {fallback_title}\nDescription: Content could not be retrieved from the URL. Please check if the URL is accessible and try again."
              
              return content

          def truncate_content(content, max_length=6000):
              """Safely truncate content to stay under token limit"""
              
              content_length = len(content)
              print(f"Original content length: {content_length} characters")
              
              if content_length > max_length:
                  truncated = content[:max_length] + "... [Content truncated to fit token limit]"
                  print(f"Content truncated from {content_length} to {max_length} characters")
                  return truncated
              else:
                  print("Content within limit, no truncation needed")
                  return content

          def set_github_output(name, value):
              """Safely set GitHub Actions output"""
              
              github_output = os.environ.get('GITHUB_OUTPUT')
              if not github_output:
                  print("Warning: GITHUB_OUTPUT not set")
                  return
              
              delimiter = "CONTENT_EOF_DELIMITER"
              with open(github_output, 'a', encoding='utf-8') as f:
                  f.write(f"{name}<<{delimiter}\n")
                  f.write(value)
                  f.write(f"\n{delimiter}\n")

          try:
              # Extract and process content
              content = extract_content()
              truncated_content = truncate_content(content)
              
              # Set output for next step
              set_github_output('content', truncated_content)
              print("Successfully set content output for AI inference")
              
          except Exception as e:
              print(f"Error: {e}")
              # Set fallback content on error
              fallback = f"URL: {os.environ.get('FALLBACK_URL', '')}\nTitle: {os.environ.get('FALLBACK_TITLE', '')}\nDescription: Error processing content."
              set_github_output('content', fallback)
              sys.exit(0)  # Don't fail the workflow, continue with fallback

          PYTHON_SCRIPT
        env:
          CRAWL_OUTCOME: ${{ steps.crawlResult.outcome }}
          RAW_RESPONSE: ${{ steps.crawlResult.outputs.response }}
          FALLBACK_URL: ${{ github.event.issue.body }}
          FALLBACK_TITLE: ${{ github.event.issue.title }}

      - name: Run AI inference
        id: inference
        uses: actions/ai-inference@v1
        with:
          max-tokens: 2000
          system-prompt-file: ${{ steps.promptFile.outputs.file }}
          prompt: |
            ${{ steps.truncateContent.outputs.content }}

      - name: Comment with AI summary
        run: |
          echo "${{ steps.inference.outputs.response }}" | gh issue comment $ISSUE_NUMBER --body-file -
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          ISSUE_NUMBER: ${{ github.event.issue.number }}
          RESPONSE: ${{ steps.inference.outputs.response }}
